---
    - Upper Bound
    - Lower
    - Average


---
    - Set of Instructions
        - is it OS Specific?

--- Types 
    - Big-O notation
        - Big-O describes how the runtime or memory grows as input size n grows.
        - O(1) â†’ Constant Time
            - The operation takes the same amount of time no matter how big n is.
            - Ex 
                - accessing an array element by index
        - O(log n) -> Logarithmic Time
            - Each step reduces the problem size by a factor (like dividing by 2).
            - We usually assume log base 2(because most algorithms like binary search divide the problem in half each step).
                - when n = 64
                    base = 2
                        64= 2^6
                        => logâ€‹(64) = 6
                    base = 8
                        64= 8^2
                        => logâ€‹(64) = 2
            - In Big-O we ignore the base because it only differ by a constant factor
            - Ex
                Binary Search
                // find element in sorted array
                while (low <= high) {
                    int mid = (low + high) / 2;
                    if (arr[mid] == target) return mid;
                    if (arr[mid] < target) low = mid + 1;
                    else high = mid - 1;
                }
        - O(sqrt(n)) ->Square Root Time
            - Runtime grows with the square root of n.
            - Happens in algorithms where you check factors/pairs
            - At n = 64: O(log n) = 6, O(âˆšn) = 8.
            - Ex
                //Checking if a number is prime
                boolean isPrime(int n) {
                    for (int i = 2; i <= Math.sqrt(n); i++) {
                        if (n % i == 0) return false;
                    }
                    return true;
                }
        - O(n)
            - traversing a list 
        - O(n log n) â†’ Linearithmic Time
            - Common in efficient sorting algorithms like Merge Sort, Quick Sort
            - 
        - O(nÂ²) â†’ Quadratic Time
            - Typical in nested loops.
            - Bubble Sort
            for (int i = 0; i < n; i++) {
                for (int j = 0; j < n; j++) {
                    // work
                }
            }
        - O(2â¿) â†’ Exponential Time
            - recursive Fibonacci (without memoization)
            int fib(int n) {
                if (n <= 1) return n;
                return fib(n-1) + fib(n-2);
            }
        - O(n!) â†’ Factorial Time
            - Extremely slow, grows faster than exponential.
            - generating all permutations of n items.
--- Quick Growth Comparison
    - How to figure out what goes in O() with example
    If n = 16
        O(1) â†’ 1
        O(log n) â†’ 4
        O(âˆšn) â†’ 4
        O(n) â†’ 16
        O(n log n) â†’ ~64
        O(nÂ²) â†’ 256
        O(2â¿) â†’ 65,536
        O(n!) â†’ ~20 trillion

    - Big-O Growth Intuition with Analogies
        O(1) â†’ grabbing one item off the shelf.
        O(log n) â†’ searching in a dictionary.
        O(âˆšn) â†’ checking factors only up to âˆšn.
        O(n) â†’ scanning everyone in a queue.
        O(n log n) â†’ sorting papers by splitting into piles.
        O(nÂ²) â†’ everyone shakes hands with everyone.
        O(2â¿) â†’ all party schedules.
        O(n!) â†’ all seating arrangements.
--- In real-world coding
    - O(1), O(log n), O(n), O(n log n) are usually acceptable.
    - O(nÂ²) is okay for small inputs (n â‰¤ 10k).
    - O(2â¿), O(n!) are only usable for very small n.
    - 
        O(1), O(log n), O(âˆšn) grow very slowly.
        O(n), O(n log n) grow steadily and are practical for large inputs.
        O(nÂ²) gets big quickly.
        O(2â¿), O(n!) explode ðŸ”¥ even for small n.